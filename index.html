
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>VIALM: Visually Impaired Assistance with Large Models</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta name="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="900">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://inner-monologue.github.io/"/>
    <meta property="og:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta property="og:description" content="Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta name="twitter:description" content="Project page for Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />
    <meta name="twitter:image" content="https://innermonologue.github.io/img/teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <font size="+4"><b>VIALM</b>:</font> </br> Visually Impaired Assistance with Large Models </br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
               <ul class="list-inline">
                <br>

<li>Yi Zhao</li> <li>Yilin Zhang</li> <li>Rong Xiang</li> <li>Jing Li</li> <li>Hillming Li</li>

              
                <br><br>
                    <a href="https://www.polyu.edu.hk/en/comp/">
                    <img src="img/polyu_logo.jpeg" height="40px">Dept. of Computing, The HK PolyU</a> <br>
                    <!-- <h5> * Equal contribution and listed in alphabetical order. </h5> -->
                </ul>
            </div>
        </div>

        
        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2402.01735">
                            <image src="img/paper_icon.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/yqWX86uT5jM?si=pzIYuHm6_z25LSrY">
                                <image src="img/youtube_icon.png" height="60px">
                                    <h4><strong>Video</strong></h4>
                                </a>
                            </li>
                            <!-- li>
                                <a href="https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html">
                                    <image src="img/google-ai-blog-small.png" height="60px">
                                        <h4><strong>Blogpost</strong></h4>
                                    </a>
                                </li -->
                                <!-- <li>
                                    <a href="https://github.com/">
                                        <image src="img/github.png" height="60px">
                                            <h4><strong>Code</strong></h4>
                                        </a>
                                    </li> -->
                                </ul>
                                
                            </div>
                        </div>
                        
                        
                        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/vlalm_teaser.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                                
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Visually Impaired Assistance (VIA) aims to automatically help the visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (VIALM). In this task, given an image illustrating the physical environments and a linguistic request from a VI user, VIALM aims to output step-by-step guidance to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities in VIA. The results indicate that while LMs can potentially benefit VIA, their output cannot be well environment-grounded (i.e., 25.7% GPT-4's responses) and lacks fine-grained guidance (i.e., 32.1% GPT-4's responses).
                </p>

                <!-- <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/im_teaser_compressed.mp4" type="video/mp4">
                   </video>
                </div> -->
            
            </div>
        </div>

	    </div>
        </div>
            
     <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly="" style="display: none;">@misc{zhao_vialm_2024,
    title={VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models},
    author={Yi Zhao, Yilin Zhang, Rong Xiang, Jing Li, Hillming Li},
    booktitle={https://arxiv.org/abs/2402.01735},
    year={2024}
}</textarea>
            </div>
     </div>

     <div class="col-md-8 col-md-offset-2">
        <!--
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
The authors would like to thank Kanishka Rao and Vincent Vanhoucke for valuable feedback and discussions. In addition, the authors would like to acknowledge the large team who built SayCan, upon which we construct our Kitchen Mobile Manipulation experiments.
                    <br><br> -->
                Created on 14/02/2024. The website template was borrowed from <a href="https://innermonologue.github.io/">Inner Monologue</a>.
                </p>
            </div>

    </div>
</body>
</html>
